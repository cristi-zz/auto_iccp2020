{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sim0 experiments\n",
    "\n",
    "Take some random data that have similar feature/target shapes as real data. \n",
    "\n",
    "The target is just one of the inputs combined with some of the known future information\n",
    "\n",
    "The last feature in the input shows which of the other features are produced at the output.\n",
    "For a time length $n$, $f+1$ features, $k < f$ indicator variable, the $f$'th feature will have the portion $\\left \\lfloor k*(n/f) \\right \\rfloor - \\left \\lfloor (k+1)*(n/f) \\right \\rfloor$ biased with a positive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next line to get interactive graphics\n",
    "# %matplotlib widget \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from fastai2.vision.all import *\n",
    "import torch\n",
    "import network_definitions\n",
    "import seaborn as sns\n",
    "\n",
    "base_image_path = Path(\"../images\")\n",
    "base_image_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the data generator.\n",
    "def gen_data(past_length, future_length, feature_count=14, label=None):\n",
    "    # The data shape is F, S, where F is the number of features, S is the time length (past_length). T is future_length\n",
    "    # What of the F-1 features will be outputted?\n",
    "    if label is None:\n",
    "        label = np.random.randint(features-1)\n",
    "    # Generate a bunch of random data\n",
    "    features = np.random.rand(feature_count, past_length) \n",
    "    # This will encode the indicator feature\n",
    "    label_enc = np.zeros(past_length)\n",
    "    label_len = min(int(past_length/feature_count), 10)\n",
    "    label_pos = label * label_len\n",
    "    label_enc[label_pos:(label_pos+label_len)] = 2\n",
    "    # We add the label encoding to the last feature. We allow some noise\n",
    "    features[-1, :] = features[-1, :] * 0.1 + label_enc * 0.8\n",
    "    # Some known future, again, random\n",
    "    x2 = np.random.rand(future_length)\n",
    "    # The target is the linear combination between some past data and known future\n",
    "    output = features[label, 0:future_length] * 0.5 + x2 * 0.5\n",
    "    return features, x2, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see some plots.\n",
    "label=1\n",
    "x1, x2, y = gen_data(30, 10, feature_count=4, label=label)\n",
    "fig, ax = plt.subplots(6,1,figsize=(8, 8))\n",
    "ax[0].plot(x1[0, :])\n",
    "ax[0].set_title(\"Feature 0\")\n",
    "ax[1].plot(x1[1, :])\n",
    "ax[1].set_title(\"Feature 1\")\n",
    "ax[2].plot(x1[2, :])\n",
    "ax[2].set_title(\"Feature 2\")\n",
    "ax[3].plot(x1[3, :])\n",
    "ax[3].set_title(\"Indicator feature\")\n",
    "ax[4].plot(x2)\n",
    "ax[4].set_title(\"Known future\")\n",
    "ax[5].plot(y)\n",
    "ax[5].set_title(\"Target\")\n",
    "print(f\"X1 shape: {x1.shape}, X2 shape: {x2.shape}\\nData label: {label}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom made data itemizer\n",
    "\n",
    "class Sim0Itemizer(ItemTransform):\n",
    "    def __init__(self, transformer=False):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def encodes(self, input):\n",
    "        x1, x2, output = input\n",
    "        if self.transformer:\n",
    "            out_shift = np.zeros(x2.shape[0])\n",
    "            out_shift[1:] = output[0:-1]\n",
    "            x2 = np.vstack([x2, out_shift])\n",
    "        x1 = tensor(x1).float()\n",
    "        x2 = tensor(x2).float()\n",
    "        y = tensor(output).float()\n",
    "        return (x1, x2), y\n",
    "    \n",
    "past_length = 30\n",
    "no_features = 4\n",
    "future_length = 10\n",
    "N = 1000\n",
    "batch_size = 64\n",
    "train_labels = np.random.randint(0, no_features - 1, size=N)\n",
    "val_labels = np.random.randint(0, no_features - 1, size=N)\n",
    "\n",
    "train_set = [gen_data(past_length, future_length, no_features, l) for l in train_labels]\n",
    "val_set = [gen_data(past_length, future_length, no_features, l) for l in val_labels]\n",
    "\n",
    "data_itemizer = Sim0Itemizer()\n",
    "tls_train = TfmdLists(train_set, data_itemizer)\n",
    "tls_valid = TfmdLists(val_set, data_itemizer)\n",
    "dloader = DataLoaders.from_dsets(tls_train, tls_valid, bs=batch_size).cuda()\n",
    "\n",
    "data_itemizerT = Sim0Itemizer(transformer=True)\n",
    "tls_trainT = TfmdLists(train_set, data_itemizerT)\n",
    "tls_validT = TfmdLists(val_set, data_itemizerT)\n",
    "dloaderT = DataLoaders.from_dsets(tls_trainT, tls_validT, bs=batch_size).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loops\n",
    "We train each model independently so we can \"tune\" if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ReduceLROnPlateau(patience=5, factor=2),\n",
    "             EarlyStoppingCallback(patience=15),\n",
    "             SaveModelCallback(fname=\"best_model\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Linear= network_definitions.LinearModel(in_features=no_features, past_steps=past_length, future_steps=future_length).cuda()\n",
    "learner_Linear = Learner(dloader, model_Linear, loss_func=MSELossFlat())\n",
    "# print(learner_Linear.summary())\n",
    "# learner_Linear.lr_find()\n",
    "learner_Linear.fit_one_cycle(10, 5e-3, cbs=callbacks)\n",
    "fig = plt.figure(figsize=(4, 2))  # because jupyter widgets always changing semantics.\n",
    "learner_Linear.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Cnn= network_definitions.CnnFcn(in_features=no_features, past_steps=past_length, future_steps=future_length, cnn_stack_len=1,\n",
    "                                fcn_stack_len=1, cnn_filters=64, avg_pool_len=10, fcn_ratio=0.8).cuda()\n",
    "learner_Cnn = Learner(dloader, model_Cnn, loss_func=MSELossFlat())\n",
    "# print(learner_Cnn.summary())\n",
    "# learner_Cnn.lr_find()\n",
    "learner_Cnn.fit_one_cycle(10, 5e-3, cbs=callbacks)\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "learner_Cnn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EncDec= network_definitions.EncoderDecoder(in_features=no_features, past_steps=past_length, future_steps=future_length, \n",
    "                                                hidden_size=128, num_layers=2).cuda()\n",
    "learner_EncDec = Learner(dloader, model_EncDec, loss_func=MSELossFlat())\n",
    "# print(learner_EncDec.summary())\n",
    "# learner_EncDec.lr_find()\n",
    "learner_EncDec.fit_one_cycle(10, 5e-3, cbs=callbacks)\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "learner_EncDec.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Att= network_definitions.AttentionModel(in_features=no_features, past_steps=past_length, future_steps=future_length, \n",
    "                                                hidden_size=128, num_layers=2).cuda()\n",
    "learner_Att = Learner(dloader, model_Att, loss_func=MSELossFlat())\n",
    "# print(learner_Att.summary())\n",
    "# learner_Att.lr_find()\n",
    "learner_Att.fit_one_cycle(40, 5e-4, cbs=callbacks)\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "learner_Att.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_TransS= network_definitions.SimpleTransformer(in_features=no_features, past_steps=past_length, future_steps=future_length, \n",
    "                                                d_model=128, dim_feedforward=512).cuda()\n",
    "learner_TransS = Learner(dloader, model_TransS, loss_func=MSELossFlat())\n",
    "# print(learner_TransS.summary())\n",
    "# learner_TransS.lr_find()\n",
    "learner_TransS.fit_one_cycle(15, 1e-4, cbs=callbacks)\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "learner_TransS.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_TransF= network_definitions.TransformerFull(in_features=no_features, past_steps=past_length, future_steps=future_length, \n",
    "                                                  target_features=2, d_model=128, dim_feedforward=512).cuda()\n",
    "learner_TransF = Learner(dloaderT, model_TransF, loss_func=MSELossFlat())\n",
    "# print(learner_TransF.summary())\n",
    "# learner_TransF.lr_find()\n",
    "learner_TransF.fit_one_cycle(15, 1e-4, cbs=callbacks)\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "learner_TransF.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's view some results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lerner_prediction(learner, sample, label):\n",
    "    decoded_output, _, _ = learner.predict(sample, with_input=False)\n",
    "    return (decoded_output.numpy(), label)\n",
    "\n",
    "item = val_set[556]\n",
    "\n",
    "predictions =[\n",
    "    get_lerner_prediction(learner_Linear, item, \"Linear\"),\n",
    "    get_lerner_prediction(learner_Cnn, item, \"CnnFcn\"),\n",
    "    get_lerner_prediction(learner_EncDec, item, \"Encoder-Decoder\"),\n",
    "    get_lerner_prediction(learner_Att, item, \"Attention\"),\n",
    "    get_lerner_prediction(learner_TransS, item, \"TransS\"),\n",
    "    get_lerner_prediction(learner_TransF, item, \"TransF\"),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 5))\n",
    "ax[0].plot(item[0].T)\n",
    "ax[0].set_title(\"x1 features\")\n",
    "ax[1].plot(item[1])\n",
    "ax[1].set_title(\"x2, knwon future\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(base_image_path / \"sim0_sample_data.png\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4));ax=[ax]\n",
    "ax[0].plot(item[2], 'r-.', label=\"GT\",)\n",
    "for preds in predictions:\n",
    "    ax[0].plot(preds[0], label=preds[1])\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].set_title(\"Predictions for each model\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(base_image_path / \"sim0_sample_predictions.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
